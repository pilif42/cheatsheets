- Amount of data processed each day in PROD:
      - 30 million events
      - 175 million indexes
      - Both are currently loaded onto Kafka topics via batch processes (ie Spring Boot apps reading files).


- Kafka setup in PROD: 3 Kafka brokers and only 1 partition per topic. This limit of 1 has been spotted by the Hortonworks consultants as a bad bottleneck. It should be increased but be aware that:
      - More partitions mean more files opened -> more memory requirement.
      - More parallelism means more processing -> more memory requirement.


- To verify the performance of a Storm topology, open the Storm UI and check in the following order:
      - Kafka Spouts lag.
      - Processing times at each bolt.
      - Capacity of each bolt. The maximum figure is 1. The recommended figure by Hortonworks is 0.85.


- To improve the performance of a Storm topology, do the following one step at a time. After each step, you need to restart your topology and verify the capacity of each bolt (is it closer to .85?):
      - Add more partitions to the Kafka topic our Storm app reads from.
      - Increase topology.max.spout.pending. The default value is 1000 and it means that the Kafka spout reads 1000 messages at a time.


- To install and start topologies:
      - ssh into the Edge node and cd /home/app/
      - wget -O install.tar "https://nexus.sample.com:8443/service/local/artifact/maven/redirect?r=snapshots&g=com.sample&a=storm-ingest&v=1.0.857-SNAPSHOT&p=tar"
      - tar -xvf install.tar
      - rm install.tar
      - cd storm-ingest-1.0.857-SNAPSHOT
      - ./deploy.sh
      - Amend the event topology name to sth easy to track
            - cd storm-event-ingest-1.0.857-SNAPSHOT
            - cd ext-lib
            - vi configArgs.yaml
      - Amend the index topology name to sth easy to track
            - cd ../../storm-index-ingest-1.0.857-SNAPSHOT
            - cd ext-lib
            - vi configArgs.yaml
      - Verify all running topos: storm list
      - Kill running topos if required:
            - recommended way:
                - identify worker nodes (In devint, we only had 1 worker node per topology.)
                - ssh into each worker node
                - cd /home/app/
                - sh topology_cleanup.sh <Topo_ID> <Topo_Name> (Note that the content of topology_cleanup.sh is further below. Topo_ID & Topo_Name can be found in the Storm UI.)
            - or shortcut:
                - use 'storm kill' at the Edge node but not recommended as it has a tendency to leave behind running processes in worker nodes:
                - storm kill event_topology
                - storm kill index_topology
      - Start the event topo:
            - cd storm-event-ingest-1.0.857-SNAPSHOT
            - ./start.sh


- Storm issue when using Java 9 classes
    - Version of Storm: from within HDP 2.6.5, ie 1.1.0.2.6.5.41-2
    - Issue when starting the topology in the edge node:
          Exception in thread "main" java.lang.IllegalArgumentException
          at org.apache.storm.hack.shade.org.objectweb.asm.ClassReader.<init>(Unknown Source)
          at org.apache.storm.hack.shade.org.objectweb.asm.ClassReader.<init>(Unknown Source)
          â€¦.
          Error: Could not find or load main class com.sample.storm.EventIngestStormTopology
    - Solution: revert Lombok from 1.18.8 to 1.16.20
    - Reason: The Hortonworks installation of storm is configured by default to perform transformation of class files in legacy jars during topology submission. The transformation step is controlled by a property in storm.yaml with the key client.jartransformer.class and the default value for Hortonworks is org.apache.storm.hack.StormShadeTransformer
      This transformation step uses the ASM library to package-relocate classes built against legacy version of storm (predating 1.0). The version of ASM that is used to do this only supports class files built against Java 8 and earlier. If a topology jar contains a class with a version of Java 9 or newer, the transformation step will fail with the following output.
      Exception in thread "main" java.lang.IllegalArgumentException
      at org.apache.storm.hack.shade.org.objectweb.asm.ClassReader.<init>(Unknown Source)
      at org.apache.storm.hack.shade.org.objectweb.asm.ClassReader.<init>(Unknown Source)
      at org.apache.storm.hack.shade.org.objectweb.asm.ClassReader.<init>(Unknown Source)
      Java 9 introduced a new feature called Multi-Release JAR Files under JEP 238 http://openjdk.java.net/jeps/238 and recent versions of libraries such as ElasticSearch have started to build multi-release jars that contain classes with versions newer than Java 9. Due to the aformentioned lack of support for Java 9 and newer, the transformation step fails and prevents the submission of topologies.
      Storm can be configured to skip the transformation step by either removing the client.jartransformer.class from storm.yaml or setting its value to 'nil'. The former does not appear to be possible with Hortonworks distributions since a non-empty value is required.
      https://gitlab.ow2.org/asm/asm/blob/ASM_5_0_2/src/org/objectweb/asm/ClassReader.java#L169-17
      https://github.com/hortonworks/storm-release/blob/HDP-2.6.5.3008-9-tag/bin/storm.py#L308
      I am recommending setting the value to 'nil' in future ACS releases to support topologies that have dependencies on multi-release jars.
      In our case, so we do not have to modify the Storm set up, I reverted to an older version of Lombok as it was good enough for what we needed to achieve at the time.


- To verify which classes are part of your uber jar:
     jar tf my-storm-app-1.0.0-SNAPSHOT-repackaged.jar > tempo.txt


- Location of properties files relied upon by libraries added as compile dependencies:
    - When I explode my-storm-app-1.0.0-SNAPSHOT-repackaged.jar, these props files (kafka.producer.default.properties, event-config.yml, etc.) are directly at the root.


- Example of a Topology:
    - define in a topology class:
          - spout
          - decoder bolt
          - filter bolt
          - hbase bolt
          - serializerBolt
          - kafkaBolt to publish to Kafka


- General info:
    - to attach bolts together: shuffleGrouping
    - important methods in a bolt: prepare, execute, emit, declareOutputFields
        - order in which they are called: https://stackoverflow.com/questions/28981197/execution-flow-of-a-storm-program
    - to pass through a tuple:
        - when you emit from one bolt: use the input tuple as an anchor
        - do not forget to ack the tuple at the end of the processing: done with this.outputCollector.ack(tuple); at the end of a bolt's execute method.


- topology_cleanup.sh (script to kill topologies):

#!/usr/bin/sh
#################################################################################
# SCRIPT_NAME: topology_cleanup.sh
#################################################################################

validateArguments() {
    echo "Topology ID is $1"
    TOPOLOGY_ID=$1
    if [ ${#TOPOLOGY_ID} -le 14 ]
        then
    ## The Topology id is uniquely identified by 14 additional characters after the topology name, if id is less than 14
    ## the script will reconfirm with the user.
        echo "The topology id seems to be incorrect"
        echo "Wrong id can kill processes you dont want to kill and cause unexpected system behaviour"
        echo "To exit the script type 'No' and press enter/return"
        echo "To continue type 'Yes'(case sensitive) and press enter/return"
        read ANSWER
        if [ -z "$ANSWER" ]
        then
          exit 1;
        fi
          if [ $ANSWER != "Yes" ]
          then
            echo "exiting the script"
            exit 1
          fi
        fi
}

topologyCleanup() {
    TOPOLOGY_ID=$1
    TOPOLOGY_NAME=$2
    storm kill ${TOPOLOGY_NAME} -w 20
    STORM_PROCESSES=$(ps -ef | grep ${TOPOLOGY_ID} | grep -v 'grep' | awk '{ printf " " $2 }')
    if [ -z "${STORM_PROCESSES}" ]
    then
      echo "No process found to be killed for topology"
    else
      echo "Processes to be killed for the topology are:"
      echo ${STORM_PROCESSES}
      echo "Killing these processes....."
      kill -9 ${STORM_PROCESSES}
    fi
}

main() {
TOPOLOGY_ID=$1
TOPOLOGY_NAME=$2
validateArguments ${TOPOLOGY_ID}
topologyCleanup ${TOPOLOGY_ID} ${TOPOLOGY_NAME}
}

#############################################
#Execution of this shell script starts here
#############################################
echo "*** NOTE: This script should be executed inside a Storm Worker Node ***"
if [ $# -ne 2 ]
  then
    echo "Incorrect number of arguments supplied, add topology id and topology name as arguments to this script"
    echo "Usage: sh topology_cleanup.sh <topology_id> <topology_name>"
    echo "Example: sh topology_cleanup.sh events_topology_155-11-1550252858 events_topology_155"
    exit 1
fi

main $1 $2
